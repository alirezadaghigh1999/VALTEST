# VALTEST: Automated Validation of LLM-Synthesized Test Cases

<div align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-blue.svg" alt="Python 3.8+">
  <img src="https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg" alt="License: CC BY 4.0">
  <img src="https://img.shields.io/badge/Framework-Semantic%20Entropy-orange.svg" alt="Framework: Semantic Entropy">
</div>

<p align="center">
  <img src="https://raw.githubusercontent.com/username/VALTEST/main/docs/images/valtest_overview.png" alt="VALTEST Framework Overview" width="650">
  <br>
  <em>VALTEST Framework: Using semantic entropy to validate LLM-generated test cases</em>
</p>

## ğŸ” Overview

VALTEST is a robust framework for **automatically validating test cases generated by Large Language Models (LLMs)**. By leveraging token probabilities to predict test case validity, VALTEST significantly improves the reliability of LLM-generated test cases. Our framework evaluates the validity, mutation score, and coverage metrics across three popular datasets (HumanEval, MBPP, and LeetCode) and multiple LLMs (GPT-4o, GPT-3.5-turbo, and LLama3).

> **Note**: The image above is a placeholder. To make this README more professional, you should create a diagram showing the VALTEST workflow and replace the URL with the actual image path.

## ğŸ“Š Key Results

<p align="center">
  <img src="https://raw.githubusercontent.com/username/VALTEST/main/docs/images/comparison_chart.png" alt="Performance Comparison" width="600">
  <br>
  <em>Performance comparison of different models and approaches</em>
</p>

Our evaluation shows that **VALTEST** consistently outperforms baseline approaches:

| Metric | VALTEST | Baseline | Improvement |
|--------|---------|----------|-------------|
| Validity Rate | 92.6% | 78.3% | +14.3% |
| Mutation Score | 85.7% | 70.1% | +15.6% |
| Line Coverage | 89.3% | 76.5% | +12.8% |

> **Note**: Replace the placeholder image and metrics with actual data from your experiments.

## ğŸ› ï¸ Project Structure

```
VALTEST/
â”œâ”€â”€ main_train.py          # Main training script
â”œâ”€â”€ generate_testcases.py  # Test case generation from LLMs
â”œâ”€â”€ curate_testcases.py    # Test case refinement and validation
â”œâ”€â”€ function_executor.py   # Executes functions with test cases
â”œâ”€â”€ llm_requester.py       # Interface with LLM APIs
â”œâ”€â”€ loaders/               # Dataset loaders
â”œâ”€â”€ models/                # Trained prediction models
â”œâ”€â”€ filtered_testcases/    # Validated test cases
â””â”€â”€ output/                # Evaluation results
```

## ğŸ§© Key Components

### Semantic Entropy Analysis

<p align="center">
  <img src="https://raw.githubusercontent.com/username/VALTEST/main/docs/images/semantic_entropy.png" alt="Semantic Entropy Analysis" width="550">
  <br>
  <em>Visualization of semantic entropy metrics for valid vs. invalid test cases</em>
</p>

The framework extracts statistical features from token probabilities for both function inputs and expected outputs:
- Mean, max, min, sum, variance metrics
- Token counts for top and second-predicted tokens
- Entropy distribution patterns that correlate with test validity

### Machine Learning Pipeline

Our ML pipeline predicts test case validity using:
- Feature extraction from token probability distributions
- Cross-validated model training (logistic regression, random forest, ensemble)
- Threshold-based selection of high-confidence test cases

### Test Case Generation & Curation

- **Generation**: LLMs produce test cases from function signatures
- **Validation**: Semantic entropy metrics predict validity
- **Curation**: Chain-of-thought reasoning corrects invalid test cases

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- OpenAI API key (for GPT-4o, GPT-3.5-turbo)
- Fireworks API key (for CodeQwen)
- ~25GB VRAM (for LLama3)

### Installation

```bash
# Clone the repository
git clone https://github.com/username/VALTEST.git
cd VALTEST

# Install dependencies
pip install -r requirements.txt
```

### Environment Setup

Create a `.env` file in the project root with your API keys:

```
# For OpenAI models
openai_key=your_openai_api_key

# For CodeQwen
fireworks_key=your_fireworks_api_key
```

For BigCodeBench experiments, create a separate Python environment:

```bash
python -m venv .bigcode_venv
source .bigcode_venv/bin/activate
pip install -r requirements_bigcode.txt
```

## ğŸ“ Usage

### Generating Test Cases

```bash
python generate_testcases.py --dataset HumanEval --llm gpt-4o
```

**Supported datasets**: `MBPP`, `HumanEval`, `LeetCode`, `BigCodeBench`, `BigCodeBenchHard`  
**Supported LLMs**: `gpt-4o`, `gpt-3.5-turbo`, `llama3`, `codeqwen`

### Training the Validation Model

```bash
python main_train.py --dataset HumanEval --llm gpt-4o --mutation 0 --threshold 0.8 --topN 5 --features all
```

### Curating and Evaluating Test Cases

```bash
# Curate test cases
python curate_testcases.py --dataset HumanEval --llm gpt-4o

# Evaluate with mutation testing
python main_train.py --dataset HumanEval --llm gpt-4o --mutation 1
```

## ğŸ“‹ Parameters

| Parameter | Description | Options | Default |
|-----------|-------------|---------|---------|
| `--dataset` | Dataset to use | `MBPP`, `HumanEval`, `LeetCode`, `BigCodeBench`, `BigCodeBenchHard` | Required |
| `--llm` | LLM to use | `gpt-4o`, `gpt-3.5-turbo`, `llama3`, `codeqwen` | Required |
| `--mutation` | Enable mutation testing | `0` (disable), `1` (enable) | `0` |
| `--threshold` | Minimum probability for valid tests | `0.5` - `0.9` | `0.8` |
| `--topN` | Number of top tests per function | `1`, `3`, `5`, `7` | `3` |
| `--features` | Feature sets to use | `all`, `input`, `output` | `all` |

## ğŸ“Š Results and Evaluation

Results are stored in `output/{dataset}_{llm}_{approach}.txt` with the following metrics:

- **Validity Rate (VR)**: Proportion of valid test cases
- **Mutation Score (MS)**: Percentage of killed mutants (fault-detection capability)
- **Line Coverage (LC)**: Code coverage percentage

<p align="center">
  <img src="https://raw.githubusercontent.com/username/VALTEST/main/docs/images/metrics_comparison.png" alt="Metrics Comparison" width="600">
  <br>
  <em>Comparison of metrics across different datasets and models</em>
</p>

## ğŸ“„ License

This project is licensed under the **Creative Commons Attribution 4.0 International (CC BY 4.0) License**.

You are free to:
- **Share** â€” Copy and redistribute the material in any medium or format.
- **Adapt** â€” Remix, transform, and build upon the material for any purpose, even commercially.

Under the following terms:
- **Attribution** â€” You must give appropriate credit, provide a link to the license, and indicate if changes were made.
- **No additional restrictions** â€” You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.

[View Full License](https://creativecommons.org/licenses/by/4.0/)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“š Citation

If you use VALTEST in your research, please cite our paper:

```bibtex
@article{valtest2023,
  title={Toward Automated Validation of Language Model Synthesized Test Cases using Semantic Entropy},
  author={Author, A. and Author, B.},
  journal={Proceedings of...},
  year={2023}
}
```

## ğŸ“§ Contact

For questions or support, please open an issue or contact [email@example.com](mailto:email@example.com).
